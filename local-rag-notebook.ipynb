{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "S41XMbZCJRPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1R6HrGnGeVH"
      },
      "outputs": [],
      "source": [
        "!pip install lancedb langchain langchain_community pypdf requests numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Extracting the relevant information\n",
        "\n",
        "First, we'll load the local PDF file you specified (CPG.pdf)."
      ],
      "metadata": {
        "id": "CoeHmze8Jkul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the local PDF file\n",
        "pdf_path = \"CPG.pdf\"  # Update this path if your PDF is in a different location\n",
        "pdf_loader = PyPDFLoader(pdf_path)\n",
        "docs = pdf_loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} pages from {pdf_path}\")"
      ],
      "metadata": {
        "id": "mxQwf4pLJjxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Breaking the information into smaller chunks\n",
        "\n",
        "Now we'll split the PDF content into smaller chunks for better processing and retrieval."
      ],
      "metadata": {
        "id": "oY52MPWWKUH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ],
      "metadata": {
        "id": "ZJX-NOLwKYQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Creating the embeddings and storing them in a vector database\n",
        "\n",
        "We'll use a sentence transformer model to create embeddings for our text chunks and store them in a LanceDB database."
      ],
      "metadata": {
        "id": "8NmWSIRmKZz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a lightweight but effective embedding model\n",
        "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})\n",
        "\n",
        "# Check embedding dimensions\n",
        "test_query = \"Test embedding dimensions\"\n",
        "embedding_dim = len(embeddings.embed_query(test_query))\n",
        "print(f\"Embedding dimension: {embedding_dim}\")"
      ],
      "metadata": {
        "id": "pg645dbSH32_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing the embeddings to a vector database"
      ],
      "metadata": {
        "id": "_V8Bdjn_LKSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lancedb\n",
        "from langchain_community.vectorstores import LanceDB\n",
        "\n",
        "# Create a LanceDB database\n",
        "db = lancedb.connect(\"cpg_lance_db\")\n",
        "\n",
        "# Initialize the table with a sample\n",
        "table = db.create_table(\n",
        "    \"cpg_data\",\n",
        "    data=[\n",
        "        {\n",
        "            \"vector\": embeddings.embed_query(\"Initialization vector\"),\n",
        "            \"text\": \"Initialization vector\",\n",
        "            \"id\": \"0\",\n",
        "        }\n",
        "    ],\n",
        "    mode=\"overwrite\",\n",
        ")\n",
        "\n",
        "# Store document chunks and their embeddings\n",
        "docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)\n",
        "print(\"Documents successfully embedded and stored in vector database\")"
      ],
      "metadata": {
        "id": "4QLp4LisLIGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create a prompt template for the LLM\n",
        "\n",
        "Let's create a comprehensive prompt template that incorporates context from our retrieved documents."
      ],
      "metadata": {
        "id": "cUdKpqIBLe7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "Context information is below:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge, answer the following question:\n",
        "Question: {query}\n",
        "\n",
        "If the answer cannot be determined from the context, say so.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "BQb0Oh5BLj-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 & 6: Set up the retriever to fetch relevant documents\n",
        "\n",
        "Now we'll configure the retriever to search for the most relevant document chunks when given a query."
      ],
      "metadata": {
        "id": "ZCv5L0-nL2jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the retriever with parameters for how many documents to fetch\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\": 4})  # Fetch top 4 most relevant chunks\n",
        "\n",
        "# Test the retriever with a sample query\n",
        "test_query = \"What are the main topics covered in this document?\"\n",
        "retrieved_docs = retriever.get_relevant_documents(test_query)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents for the test query\")\n",
        "print(\"\\nSample of first retrieved document:\")\n",
        "print(retrieved_docs[0].page_content[:200] + \"...\")"
      ],
      "metadata": {
        "id": "vjCko1mrL82o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Connect to the locally served LLM API\n",
        "\n",
        "Instead of using Hugging Face Hub, we'll connect to your locally served LLM API."
      ],
      "metadata": {
        "id": "22p5wkPMMAXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import OpenAI\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Configure your local LLM API endpoint\n",
        "# Change these parameters based on your local LLM API setup\n",
        "local_llm_url = \"http://localhost:8000/v1\"  # Update with your actual API endpoint\n",
        "api_key = \"local-api-key\"  # Use your API key or set to a dummy value if not required\n",
        "\n",
        "# Set up the LLM to use your local API\n",
        "llm = OpenAI(\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=local_llm_url,\n",
        "    streaming=True,\n",
        "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024,\n",
        "    model_name=\"local-model\"  # This can be any string that your API requires\n",
        ")"
      ],
      "metadata": {
        "id": "WheAmghxMBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative Step 7: Use LangChain's ChatOpenAI for chat models\n",
        "\n",
        "If your local LLM is a chat model rather than a completion model, you might want to use this cell instead."
      ],
      "metadata": {
        "id": "alternative_step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Uncomment and use this if your local LLM is a chat model\n",
        "'''\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=api_key,\n",
        "    openai_api_base=local_llm_url,\n",
        "    streaming=True,\n",
        "    temperature=0.3,\n",
        "    max_tokens=1024,\n",
        "    model_name=\"local-model\"  # This can be any string that your API requires\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "chat_model_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Create a chain for invoking the LLM\n",
        "\n",
        "Finally, we'll create a chain that combines our retriever, prompt template, and LLM to answer questions."
      ],
      "metadata": {
        "id": "f3gcAY-7MD9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Create a helper function to format the context from retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "htz4xZbEMJ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the RAG Application\n",
        "\n",
        "Now let's test our RAG application with some questions."
      ],
      "metadata": {
        "id": "testing_the_app"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the RAG chain with a question\n",
        "question = \"What are the main recommendations in this document?\"\n",
        "print(\"\\nQuestion:\", question)\n",
        "print(\"\\nAnswer:\")\n",
        "response = rag_chain.invoke(question)\n",
        "\n",
        "# The response is already printed via streaming due to the StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "test_question_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try another question\n",
        "question = \"What is the scope of this document?\"\n",
        "print(\"\\nQuestion:\", question)\n",
        "print(\"\\nAnswer:\")\n",
        "response = rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "test_question_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: Create a Simple Q&A Interface\n",
        "\n",
        "Let's create a simple loop to ask multiple questions interactively."
      ],
      "metadata": {
        "id": "qa_interface"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(\"\\nA: \", end=\"\")\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "# Interactive Q&A loop\n",
        "while True:\n",
        "    user_question = input(\"\\nEnter your question (or type 'exit' to quit): \")\n",
        "    if user_question.lower() == 'exit':\n",
        "        print(\"Exiting Q&A session.\")\n",
        "        break\n",
        "    \n",
        "    ask_question(user_question)"
      ],
      "metadata": {
        "id": "interactive_qa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
